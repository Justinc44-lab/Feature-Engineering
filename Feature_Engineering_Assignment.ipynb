{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a parameter ?\n",
        ">>> A parameter is a value or variable that is used to define or control the behavior of a function, method, or system. In programming, parameters are passed into functions to provide input or context for processing. For example, in a function that adds two numbers, the two numbers would be parameters.\n",
        "\n",
        "In a broader context, parameters can refer to the factors or variables that influence the operation or outcome of a process or system.\n",
        "\n",
        "2. What is correlation ? What does negative correlation mean ?\n",
        ">>> Correlation is a statistical measure that describes the relationship between two or more variables. It indicates how changes in one variable are related to changes in another.\n",
        "- **Negative correlation** means that as one variable increases, the other decreases.\n",
        "The strength of the correlation is measured by a value called the **correlation coefficient**, which ranges from -1 (perfect negative correlation) to +1 (perfect positive correlation). A value close to 0 suggests little to no correlation.\n",
        "\n",
        "3. Define Machine Learning. What are the main components in Machine Learning?\n",
        ">>> **Machine Learning (ML)** is a field of artificial intelligence (AI) that focuses on building systems that can learn from data and improve their performance over time without being explicitly programmed.\n",
        "\n",
        "### Main Components in Machine Learning:\n",
        "\n",
        "**A. Data**:  \n",
        "Raw information used to train and test the model, such as numbers, text, or images.\n",
        "\n",
        "**B. Algorithms**:  \n",
        "Mathematical methods or models used to learn patterns from data and make predictions or decisions.\n",
        "\n",
        "**C. Model**:  \n",
        "The output of a trained algorithm, which can predict or make decisions based on new data.\n",
        "\n",
        "**D. Training**:  \n",
        "The process where an algorithm learns from the data by adjusting its parameters to minimize errors.\n",
        "\n",
        "**E. Evaluation**:  \n",
        "Assessing how well the model performs using various metrics (e.g., accuracy, precision) to determine its effectiveness.\n",
        "\n",
        "**F. Deployment**:  \n",
        "Implementing the trained model in real-world applications to make predictions or automate tasks.\n",
        "\n",
        "These components work together to enable machines to learn from data and make informed decisions.\n",
        "\n",
        "4. How does loss value help in determining whether the model is good or not?\n",
        ">>> The **loss value** (or loss function) measures how well or poorly a model's predictions match the actual results (ground truth). It quantifies the error between predicted values and the true values.\n",
        "\n",
        "- **Low loss** means the model's predictions are close to the actual results, indicating good performance.\n",
        "- **High loss** indicates a larger discrepancy between predictions and actual results, meaning the model needs improvement.\n",
        "\n",
        "By monitoring the loss value during training, we can track how well the model is learning. If the loss decreases over time, the model is improving. If it remains high or increases, it suggests the model is not learning effectively and needs adjustments.\n",
        "\n",
        "5. What are continuous and categorical variables?\n",
        ">>> **Continuous Variables**:  \n",
        "These are variables that can take any value within a range and can have infinite possible values. They are typically measured and can represent things like height, weight, temperature, or time. Examples: 5.4, 10.7, 200.1.\n",
        "\n",
        "**Categorical Variables**:  \n",
        "These variables represent distinct categories or groups and have a limited number of possible values. They are often qualitative and represent things like gender, color, or type of product. Examples: \"Red,\" \"Blue,\" \"Male,\" \"Female.\"\n",
        "\n",
        "In summary:\n",
        "- **Continuous** = Numerical, infinite possibilities (e.g., age, salary).\n",
        "- **Categorical** = Grouped into categories (e.g., gender, country).\n",
        "\n",
        "6. How do we handle categorical variables in Machine Learning? What are the common techniques ?\n",
        ">>> Handling categorical variables in Machine Learning involves converting them into a numerical format that algorithms can understand. Common techniques include\n",
        "**A. Label Encoding**:  \n",
        "Converts each category into a unique integer. Useful for ordinal data where the categories have a natural order (e.g., \"Low\" = 0, \"Medium\" = 1, \"High\" = 2).\n",
        "\n",
        "**B. One-Hot Encoding**:  \n",
        "Creates a binary column for each category. Each category gets a separate column with 1 or 0 indicating its presence. Useful for nominal data without any inherent order.\n",
        "\n",
        "**C. Binary Encoding**:  \n",
        "Converts categories into binary numbers and creates separate columns for each binary digit. It's more efficient for datasets with many categories.\n",
        "\n",
        "**D. Frequency or Count Encoding**:  \n",
        "Replaces each category with the count or frequency of that category in the dataset. This technique is helpful when the frequency of categories holds predictive value.\n",
        "\n",
        "These are common ways to transform categorical data for machine learning models.\n",
        "\n",
        "7. What do you mean by training and testing a dataset?\n",
        ">>> Here’s the explanation with alphabetic pointers:\n",
        "\n",
        "**A. Training a Dataset**:  \n",
        "Training involves feeding a machine learning model with a dataset to allow it to learn patterns, relationships, and features. The model adjusts its parameters to minimize errors based on the training data.\n",
        "\n",
        "**B. Testing a Dataset**:  \n",
        "Testing is the process of evaluating the model's performance using a separate dataset (called the test set) that was not used during training. This helps assess how well the model generalizes to unseen data.\n",
        "\n",
        "In summary:\n",
        "- **Training** = Teaching the model using labeled data.\n",
        "- **Testing** = Evaluating the model’s performance on new, unseen data.\n",
        "\n",
        "8. What is sklearn.preprocessing ?\n",
        ">>> Here’s the explanation with alphabetic pointers:\n",
        "\n",
        "**A. sklearn.preprocessing**:  \n",
        "`sklearn.preprocessing` is a module in the **scikit-learn** library that provides various methods to preprocess data before feeding it into a machine learning model. It helps in transforming and scaling data to improve model performance.\n",
        "\n",
        "**B. Common Techniques in sklearn.preprocessing**:\n",
        "- **StandardScaler**: Scales features to have a mean of 0 and a standard deviation of 1 (standardization).\n",
        "- **MinMaxScaler**: Scales features to a specified range, often [0, 1].\n",
        "- **OneHotEncoder**: Converts categorical variables into a binary matrix.\n",
        "- **LabelEncoder**: Converts categorical labels into numeric values.\n",
        "- **Normalizer**: Scales each sample to have unit norm (useful for sparse data).\n",
        "\n",
        "This module helps in preparing and transforming data efficiently for machine learning models.\n",
        "\n",
        "9. What is a Test set ?\n",
        ">>> Here’s the explanation with alphabetic pointers:\n",
        "\n",
        "**A. Test Set**:  \n",
        "A **test set** is a subset of the dataset that is used to evaluate the performance of a machine learning model after it has been trained on a training set. The test set is not used during the training phase to ensure the model’s ability to generalize to unseen data.\n",
        "\n",
        "**B. Purpose**:  \n",
        "The test set helps assess how well the model performs in real-world situations by predicting outcomes for new, unseen data. This evaluation helps identify overfitting or underfitting issues.\n",
        "\n",
        "In summary:\n",
        "- **Test set** = A data subset used to evaluate model performance after training.\n",
        "\n",
        "10. How do we split data for model fitting (training and testing) in Python? How do you approach a Machine Learning problem.\n",
        ">>>\n",
        "### **How to Split Data for Model Fitting**:\n",
        "\n",
        "**A. Using `train_test_split` from `sklearn.model_selection`**:  \n",
        "The most common approach to splitting data into training and testing sets is using the `train_test_split` function. It randomly splits the dataset into two subsets: one for training and one for testing.\n",
        "\n",
        "Example:\n",
        "```python\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "```\n",
        "- `X` represents features (inputs).\n",
        "- `y` represents labels (outputs).\n",
        "- `test_size=0.2` means 20% of the data will be used for testing, and 80% will be for training.\n",
        "\n",
        "---\n",
        "\n",
        "### **How to Approach a Machine Learning Problem**:\n",
        "\n",
        "**A. Understand the Problem**:  \n",
        "Clearly define the problem you're trying to solve (e.g., classification, regression) and understand the goals.\n",
        "\n",
        "**B. Collect and Preprocess Data**:  \n",
        "Gather relevant data, clean it (handling missing values, encoding categorical variables), and scale/normalize features if needed.\n",
        "\n",
        "**C. Split the Data**:  \n",
        "Divide the data into training and test sets, ensuring a fair evaluation.\n",
        "\n",
        "**D. Choose a Model**:  \n",
        "Select an appropriate machine learning algorithm based on the problem (e.g., linear regression, decision tree, SVM).\n",
        "\n",
        "**E. Train the Model**:  \n",
        "Fit the model to the training data using the chosen algorithm.\n",
        "\n",
        "**F. Evaluate the Model**:  \n",
        "Use the test set to evaluate the model’s performance (e.g., accuracy, precision, recall, F1-score).\n",
        "\n",
        "**G. Tune Hyperparameters**:  \n",
        "Optimize the model by fine-tuning hyperparameters to improve performance.\n",
        "\n",
        "**H. Deploy the Model**:  \n",
        "Once satisfied with the model’s performance, deploy it to make predictions on new, unseen data.\n",
        "\n",
        "This is a general workflow for solving a machine learning problem.\n",
        "\n",
        "11. Why do we have to perform EDA before fitting a model to the data?\n",
        ">>> **A.** **Understand Data**:  \n",
        "EDA helps in understanding the structure, patterns, and relationships in the data.\n",
        "\n",
        "**B.** **Identify Missing Data**:  \n",
        "It allows identifying missing values or outliers that could affect model performance.\n",
        "\n",
        "**C.** **Check for Outliers**:  \n",
        "Helps in detecting anomalies that can distort the model's predictions.\n",
        "\n",
        "**D.** **Feature Distribution**:  \n",
        "Shows the distribution of features and helps decide if transformations or scaling are needed.\n",
        "\n",
        "**E.** **Correlation Analysis**:  \n",
        "Identifies correlations between features, which can guide feature selection or engineering.\n",
        "\n",
        "**F.** **Better Preprocessing**:  \n",
        "EDA informs decisions about data cleaning, normalization, or encoding necessary before model fitting.\n",
        "\n",
        "12. What is correlation?\n",
        ">>> **A.** **Definition**:  \n",
        "Correlation is a statistical measure that indicates the strength and direction of the relationship between two or more variables.\n",
        "\n",
        "**B.** **Types**:  \n",
        "- **Positive Correlation**: As one variable increases, the other also increases.\n",
        "- **Negative Correlation**: As one variable increases, the other decreases.\n",
        "- **No Correlation**: No predictable relationship between the variables.\n",
        "\n",
        "**C.** **Correlation Coefficient**:  \n",
        "The correlation is measured by a value between -1 and 1, where:\n",
        "- **+1** indicates a perfect positive correlation.\n",
        "- **-1** indicates a perfect negative correlation.\n",
        "- **0** indicates no correlation.\n",
        "\n",
        "13. What does negative correlation mean ?\n",
        ">>> **A.** **Negative Correlation**:  \n",
        "Negative correlation means that as one variable increases, the other decreases, or vice versa.\n",
        "\n",
        "**B.** **Example**:  \n",
        "If the temperature decreases, the sale of hot chocolate might increase, showing a negative correlation between temperature and hot chocolate sales.\n",
        "\n",
        "**C.** **Correlation Coefficient**:  \n",
        "In negative correlation, the correlation coefficient is between -1 and 0, where -1 indicates a perfect inverse relationship.\n",
        "\n",
        "14. How can you find correlation between variables in Python?\n",
        ">>> **A.** **Using `pandas.corr()` Method**:  \n",
        "You can use the `.corr()` method from the pandas library to compute the correlation between variables in a DataFrame.\n",
        "\n",
        "Example:\n",
        "```python\n",
        "import pandas as pd\n",
        "data = pd.DataFrame({\n",
        "    'A': [1, 2, 3, 4],\n",
        "    'B': [4, 3, 2, 1]\n",
        "})\n",
        "correlation = data.corr()\n",
        "print(correlation)\n",
        "```\n",
        "\n",
        "**B.** **Using `seaborn.heatmap()` for Visualization**:  \n",
        "You can visualize the correlation matrix using a heatmap for easier interpretation.\n",
        "\n",
        "Example:\n",
        "```python\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.heatmap(data.corr(), annot=True, cmap='coolwarm')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "This gives a matrix of correlation values, helping to understand relationships between variables.\n",
        "\n",
        "15. What is causation? Explain difference between correlation and causation with an example?\n",
        ">>> **A.** **Causation**:  \n",
        "Causation indicates that one event or variable directly causes the change in another. It shows a cause-and-effect relationship.\n",
        "\n",
        "**B.** **Difference Between Correlation and Causation**:  \n",
        "- **Correlation**: A statistical relationship between two variables where they change together, but one doesn't necessarily cause the other.\n",
        "- **Causation**: One variable directly causes a change in the other.\n",
        "\n",
        "**C.** **Example**:  \n",
        "- **Correlation**: Ice cream sales and drowning rates might increase in summer, but eating ice cream doesn’t cause drowning.\n",
        "- **Causation**: Smoking causes lung cancer, where smoking directly leads to the disease.\n",
        "\n",
        "16. What is an Optimizer? What are different types of optimizers? Explain each with an example?\n",
        ">>> **A.** **Optimizer**:  \n",
        "An optimizer is an algorithm used to minimize or maximize the objective function (usually the loss function) by adjusting the model's parameters (weights) during training in machine learning or deep learning.\n",
        "\n",
        "---\n",
        "\n",
        "**B.** **Types of Optimizers**:\n",
        "\n",
        "**1. Gradient Descent**:  \n",
        "- **Description**: Updates model parameters by moving in the opposite direction of the gradient of the loss function.\n",
        "- **Example**: In linear regression, gradient descent updates the weights to minimize the difference between predicted and actual values.\n",
        "\n",
        "**2. Stochastic Gradient Descent (SGD)**:  \n",
        "- **Description**: A variation of gradient descent where parameters are updated using a single data point at a time (instead of the whole dataset).\n",
        "- **Example**: Used in deep learning models, where it’s computationally expensive to use the full dataset.\n",
        "\n",
        "**3. Mini-batch Gradient Descent**:  \n",
        "- **Description**: A compromise between batch and stochastic gradient descent, where the dataset is divided into small batches to compute updates.\n",
        "- **Example**: Common in training neural networks where each update uses a subset of data.\n",
        "\n",
        "**4. Adam (Adaptive Moment Estimation)**:  \n",
        "- **Description**: Combines the benefits of both gradient descent and momentum, adapting the learning rate for each parameter.\n",
        "- **Example**: Used widely in deep learning, e.g., training convolutional neural networks (CNNs), because it converges faster and adapts to the problem.\n",
        "\n",
        "**5. RMSprop (Root Mean Square Propagation)**:  \n",
        "- **Description**: Adjusts the learning rate based on the average of recent gradients, helping handle noisy gradients and stabilize training.\n",
        "- **Example**: Often used in recurrent neural networks (RNNs) to improve training stability.\n",
        "\n",
        "---\n",
        "\n",
        "These optimizers help fine-tune models efficiently and ensure faster and more accurate convergence during training.\n",
        "\n",
        "17. What is sklearn.linear_model?\n",
        ">>> **A.** **sklearn.linear_model**:  \n",
        "`sklearn.linear_model` is a module in the **scikit-learn** library that provides a variety of linear models for regression and classification tasks.\n",
        "\n",
        "---\n",
        "\n",
        "**B.** **Common Models in `sklearn.linear_model`**:\n",
        "\n",
        "**1. LinearRegression**:  \n",
        "- **Description**: Performs linear regression to predict continuous target values.\n",
        "- **Example**: Predicting house prices based on features like size and location.\n",
        "\n",
        "**2. LogisticRegression**:  \n",
        "- **Description**: Used for binary or multi-class classification problems by modeling the probability of class membership.\n",
        "- **Example**: Predicting whether an email is spam or not.\n",
        "\n",
        "**3. Ridge**:  \n",
        "- **Description**: A linear regression model with L2 regularization to prevent overfitting.\n",
        "- **Example**: Used when multicollinearity is present in data.\n",
        "\n",
        "**4. Lasso**:  \n",
        "- **Description**: A linear regression model with L1 regularization, which helps in feature selection by shrinking some coefficients to zero.\n",
        "- **Example**: Feature selection in high-dimensional datasets.\n",
        "\n",
        "**5. ElasticNet**:  \n",
        "- **Description**: A combination of L1 and L2 regularization, balancing the benefits of both Ridge and Lasso.\n",
        "- **Example**: Used when there are multiple features correlated with each other.\n",
        "\n",
        "These models are efficient for solving various linear modeling tasks, both for regression and classification.\n",
        "\n",
        "18. What does model.fit() do? What arguments must be given?\n",
        ">>> **A.** **model.fit()**:  \n",
        "`model.fit()` is a method in machine learning used to train a model on a given dataset. It allows the model to learn patterns from the input data (features) and their corresponding target values (labels).\n",
        "\n",
        "---\n",
        "\n",
        "**B.** **Arguments**:\n",
        "\n",
        "a. **X** (Required):  \n",
        "   - **Description**: The input features (independent variables), typically in the form of a 2D array or DataFrame.  \n",
        "   - **Example**: A dataset with columns representing features like age, income, etc.\n",
        "\n",
        "b. **y** (Required):  \n",
        "   - **Description**: The target labels or output (dependent variable), typically in the form of a 1D array or DataFrame.  \n",
        "   - **Example**: The actual values to predict, like house prices or class labels.\n",
        "\n",
        "---\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "model.fit(X_train, y_train)\n",
        "```\n",
        "This trains the model on the training data, allowing it to learn the relationship between `X_train` (features) and `y_train` (labels).\n",
        "\n",
        "19. What does model.predict() do? What arguments must be given ?\n",
        ">>> **A.** **model.predict()**:  \n",
        "`model.predict()` is a method used to make predictions using a trained machine learning model. It takes new, unseen data and predicts the target values based on the patterns the model has learned.\n",
        "\n",
        "---\n",
        "\n",
        "**B.** **Arguments**:\n",
        "\n",
        "1. **X** (Required):  \n",
        "   - **Description**: The input features (independent variables) for the new data, typically in the form of a 2D array or DataFrame.  \n",
        "   - **Example**: A dataset with features like age, income, etc., that the model will use to predict outcomes.\n",
        "\n",
        "---\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "predictions = model.predict(X_test)\n",
        "```\n",
        "This makes predictions using the trained model on the test data (`X_test`) and returns the predicted values.\n",
        "\n",
        "20. What are continuous and categorical variables?\n",
        ">>> **A.** **Continuous Variables**:  \n",
        "- **Definition**: Variables that can take any value within a range and have infinite possible values.\n",
        "- **Example**: Height, weight, temperature, time.\n",
        "\n",
        "**B.** **Categorical Variables**:  \n",
        "- **Definition**: Variables that represent distinct categories or groups with a limited number of possible values.\n",
        "- **Example**: Gender, color, country, type of product.\n",
        "\n",
        "In summary:\n",
        "- **Continuous** = Numeric, infinite values (e.g., age, salary).\n",
        "- **Categorical** = Grouped into categories (e.g., gender, color).\n",
        "\n",
        "21. What is feature scaling? How does it help in Machine Learning?\n",
        ">>> **A.** **Feature Scaling**:  \n",
        "Feature scaling is the process of normalizing or standardizing the range of features in a dataset to ensure they are on a similar scale.\n",
        "\n",
        "---\n",
        "\n",
        "**B.** **Types of Feature Scaling**:  \n",
        "\n",
        "**Normalization (Min-Max Scaling)**:  \n",
        "Scales features to a specific range, often [0, 1].  \n",
        "- Formula: \\( \\text{X\\_norm} = \\frac{X - \\text{min}(X)}{\\text{max}(X) - \\text{min}(X)} \\)\n",
        "\n",
        "**Standardization (Z-score Scaling)**:  \n",
        "Scales features so that they have a mean of 0 and a standard deviation of 1.  \n",
        "- Formula: \\( \\text{X\\_scaled} = \\frac{X - \\mu}{\\sigma} \\)\n",
        "\n",
        "---\n",
        "\n",
        "**C.** **Why it Helps in Machine Learning**:  \n",
        "\n",
        "**Improves Model Convergence**:  \n",
        "Many algorithms (like gradient descent) perform better and converge faster when features are scaled.  \n",
        "\n",
        "**Prevents Bias**:  \n",
        "Features with larger ranges (like income) can dominate models, especially distance-based algorithms (like KNN or SVM). Scaling ensures equal importance for all features.  \n",
        "\n",
        "**Ensures Better Accuracy**:  \n",
        "Models like neural networks and SVM are sensitive to the scale of input features, so scaling can improve their performance.  \n",
        "\n",
        "22. How do we perform scaling in Python ?\n",
        ">>> **A.** **Using `StandardScaler` for Standardization**:  \n",
        "To scale features to have a mean of 0 and a standard deviation of 1, use `StandardScaler` from `sklearn.preprocessing`.\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**B.** **Using `MinMaxScaler` for Normalization**:  \n",
        "To scale features to a specific range (e.g., [0, 1]), use `MinMaxScaler` from `sklearn.preprocessing`.\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "```\n",
        "\n",
        "These methods ensure that your features are scaled and ready for machine learning models.\n",
        "\n",
        "23. What is sklearn.preprocessing ?\n",
        ">>> **A.** **sklearn.preprocessing**:  \n",
        "`sklearn.preprocessing` is a module in the **scikit-learn** library that provides tools for data preprocessing, such as scaling, transforming, encoding, and normalizing features before feeding them into a machine learning model.\n",
        "\n",
        "---\n",
        "\n",
        "**B.** **Common Functions**:  \n",
        "\n",
        "**StandardScaler**:  \n",
        "Standardizes features by removing the mean and scaling to unit variance (z-score normalization).\n",
        "\n",
        "**MinMaxScaler**:  \n",
        "Scales features to a specific range, typically [0, 1].\n",
        "\n",
        "**OneHotEncoder**:  \n",
        "Encodes categorical features into a binary matrix (one-hot encoding).\n",
        "\n",
        "**LabelEncoder**:  \n",
        "Encodes categorical labels as integers.\n",
        "\n",
        "**RobustScaler**:  \n",
        "Scales features using the median and interquartile range, making it robust to outliers.\n",
        "\n",
        "These tools help prepare data for training machine learning models efficiently.\n",
        "\n",
        "24. How do we split data for model fitting (training and testing) in Python?\n",
        ">>> **A.** **Using `train_test_split` from `sklearn.model_selection`**:  \n",
        "To split the data into training and testing sets, use the `train_test_split` function from `sklearn.model_selection`.\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "```\n",
        "\n",
        "**B.** **Arguments**:  \n",
        "- **X**: Features (independent variables).  \n",
        "- **y**: Target labels (dependent variables).  \n",
        "- **test_size**: Fraction of the data to be used for testing (e.g., 0.2 for 20%).  \n",
        "- **random_state**: Ensures reproducibility of the split.\n",
        "\n",
        "This method splits the data into training and testing sets, helping you evaluate the model's performance on unseen data.\n",
        "\n",
        "25. Explain data encoding ?\n",
        ">>> **A.** **Data Encoding**:  \n",
        "Data encoding is the process of converting categorical variables into numerical values so they can be used in machine learning models, which typically require numerical input.\n",
        "\n",
        "---\n",
        "\n",
        "**B.** **Common Types of Encoding**:\n",
        "\n",
        "**One-Hot Encoding**:  \n",
        "Converts each category into a separate binary column, with 1 indicating the presence of the category and 0 otherwise.\n",
        "\n",
        "**Label Encoding**:  \n",
        "Assigns a unique integer value to each category, typically used for ordinal data where order matters.\n",
        "\n",
        "**Binary Encoding**:  \n",
        "Converts categories into binary code and then splits them into separate columns, a more compact method compared to one-hot encoding.\n",
        "\n",
        "These techniques help machine learning algorithms understand categorical data by converting it into a format they can process."
      ],
      "metadata": {
        "id": "e_vBR9riLBHf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2AT0MnMyVIUq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}